<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【论文解读】【ICCV2019】CenterNet: Keypoint Triplets for Object Detection</title>
      <link href="2021/01/17/centernet-keypoint-triplets-for-object-detection/"/>
      <url>2021/01/17/centernet-keypoint-triplets-for-object-detection/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>论文地址：</strong><a href="https://arxiv.org/abs/1904.08189v3" target="_blank" rel="noopener">https://arxiv.org/abs/1904.08189v3</a><br><strong>代码地址：</strong><a href="https://github.com/Duankaiwen/CenterNet" target="_blank" rel="noopener">https://github.com/Duankaiwen/CenterNet.</a></p></blockquote><h2 id="1、Background"><a href="#1、Background" class="headerlink" title="1、Background"></a>1、Background</h2><h3 id="1-1、Task"><a href="#1-1、Task" class="headerlink" title="1.1、Task"></a>1.1、Task</h3><p><strong>Object Detection：</strong>找出图像中所有感兴趣的目标，确定它们的位置和类别。</p><p><strong>Idea：</strong>该论文基于One-stage的Detector——CornerNet，提出了一个新的One-stage网络框架，将检测每个Object的任务转化成检测一组triplet。此外，作者设计了两个模块：Cascade corner pooling和Center pooling。</p><ul><li>Cascade corner pooling：得到center keypoint在水平和垂直方向上的响应分值总和的最大值。</li><li>Center pooling：得到object在边界和内部个两方面的响应分值总和的最大值。</li></ul><h3 id="1-2、Problem"><a href="#1-2、Problem" class="headerlink" title="1.2、Problem"></a>1.2、Problem</h3><p><strong>Related Work</strong><br>该论文从Two-stage approaches和One-stage approaches两个方向简介了一些经典的工作。</p><ul><li><p>Two-stage approaches：将Object detection任务分成两个阶段：1、提取RoIs（Regins of Interest），2、分类和回归RoIs。经典工作有：R-CNN、SPP-Net、Faster-RCNN（RPN）、Mask-RCNN、R-FCN、Cascade R-CNN。</p></li><li><p>One-stage approches:直接分类和回归候选anchor box，无RoI的提取模块。经典工作有：YOLO系列、SSD、DSSD、R-SSD、RON、RefineDet、 CornerNet。</p></li></ul><p><strong>Drawback:</strong><br>1、Anchor-based method：在训练时需要大量的anchors，并且每个anchor的尺寸和长宽比都需要人工设计。这些anchors通常不能对齐ground-truth boxes，这不利于边界框内容的分类任务。<br>2、ConerNet（baseline）：不能充分利用Object的全局信息。</p><h2 id="2、Motivation"><a href="#2、Motivation" class="headerlink" title="2、Motivation"></a>2、Motivation</h2><p>作者相信：如果预测所得的bounding box与ground-truth box有一个很高的IoU值，那么就可以认为用于预测同一类别的center keypoint在中心区域的概率是很高的，反之亦然。</p><p>推理阶段：在得到一对corner keypoint后，通过检查是否有一种与其同一类别的center keypoint落在该proposal的中心区域，来判断proposal内是否真的有一个object。</p><p><img src="1.png" alt=""></p><p><strong>结论：</strong>CornerNet无法利用bounding box内部区域的信息。</p><p>由此可得<strong>作者主要的motivation</strong>如下：</p><ul><li>作者将Object Detection任务转换为Keypoint Triplets估计任务。</li><li>作者根据ConerNet的缺陷，又提出了Cascade corner pooling模块和Center pooling模块，用于充分利用bounding box边界和内部的信息。</li></ul><h2 id="3、Method"><a href="#3、Method" class="headerlink" title="3、Method"></a>3、Method</h2><h3 id="3-1-Object-Detection-as-Keypoint-Triplets"><a href="#3-1-Object-Detection-as-Keypoint-Triplets" class="headerlink" title="3.1 Object Detection as Keypoint Triplets"></a>3.1 Object Detection as Keypoint Triplets</h3><p><img src="2.png" alt="模型结构图"></p><p><strong>Procedure:</strong></p><p>1、首先根据它们的分数选择top-k个center keypoints。<br>2、使用对应的offsets来把这些center keypoint重新映射在输入图像上。<br>3、对每个bounding box定义一个中心区域，并且检查中心区域内是否含有center keypoints。经过检查的center keypoint的类别标签应该和它的bounding box的类别标签相同。<br>4、如果在中心区域检测到center keypoint，那么就保留bounding box。用top-left corner、bottom-right corner和center keypoint三个点的平均分数代替bounding box的分数。</p><p><strong>自适应bounding box尺寸的中心区域的设计：</strong><br>$(tl_x, tl_y)$表示bounding box的左上角坐标。$(br_x,br_y)$表示bounding box的右下角坐标。$(ctl_x, ctl_y)$表示center region的左上角坐标。$(cbr_x,cbr_y)$表示center region的右下角坐标。<br>$$\left\{<br>\begin{array}<br>ctl_x=\frac{(n+1)tl_x + (n-1)br_x}{2n} \\<br>ctl_y=\frac{(n+1)tl_y + (n-1)br_y}{2n} \\<br>cbr_x=\frac{(n-1)tl_x + (n+1)br_x}{2n} \\<br>cbr_y=\frac{(n-1)tl_y + (n+1)br_y}{2n}<br>\end{array}<br>\right.<br>$$</p><p><img src="3.png" alt="不同n对应的bounding box和center region图"></p><p>$n$是一个奇数，取值为3或5，表示center region的尺寸。</p><h3 id="3-2、Enriching-Center-and-Corner-Information"><a href="#3-2、Enriching-Center-and-Corner-Information" class="headerlink" title="3.2、Enriching Center and Corner Information"></a>3.2、Enriching Center and Corner Information</h3><p><strong>Center pooling</strong><br>存在的问题是object的几何中心点不一定传达最具有识别性的信息（例如：人体头部包含很多视觉信息，具有很强的识别性，但center keypoint经常落在人体躯干中心）。</p><p>作者提出的方法：首先在backbone输出的特征图中判断出center keypoint的像素点，然后找到该像素点的水平和垂直方向的最大值并相加。</p><p><strong>Cascade corner pooling</strong><br>存在的问题是corners经常在objects外，这缺失了局部外观特征。CornerNet中的corner pooling是在边界上找到最大值来确定Corners，但这种方法依赖于边界，仍无法充分利用object的视觉信息。</p><p>作者提出的方法：<br>1、首先沿着边界找到边界上的最大值。<br>2、然后在这个边界最大值的位置上，向object内部搜索（上边界最大值的搜索方向：垂直向下、下边界最大值的搜索方向：垂直向上、左边界最大值的搜索方向：水平向右、右边界最大值的搜索方向：水平向左），找到内部的最大值。<br>3、最后将这两个最大值相加。<br>PS：四条边界四个点的最大值都如上所述方法可得，然后得到一对corners</p><p><img src="4.png" alt="Corner pooling、Center pooling和Cascade corner pooling图"></p><p>Center pooling和Cascade corner pooling的结构借鉴了Corner pooling结构而来，结构图如下：</p><p><img src="5.png" alt="Center pooling和Cascade corner pooling的结构图"></p><h3 id="3-3、Tasks"><a href="#3-3、Tasks" class="headerlink" title="3.3、Tasks"></a>3.3、Tasks</h3><p>Training Loss如下：<br>$$L=L_{det}^{co}+L_{det}^{ce}+\alpha L_{pull}^{co}+\beta L_{push}^{co}+\gamma (L_{off}^{co}+L_{off}^{ce})$$</p><p>$L_{det}^{co}、L_{det}^{ce}$:为focal loss，用于训练网络检测corners和center keypoints。</p><p>$L_{pull}^{co}$:”pull” loss，最小化属于相同objects的词向量的距离。</p><p>$L_{push}^{co}$:”push” loss，最大化属于不同objects的词向量的距离。</p><p>$L_{off}^{co}+L_{off}^{ce}$:用于训练网络预测corners和center keypoints的offsets。</p><h2 id="4、Experiments"><a href="#4、Experiments" class="headerlink" title="4、Experiments"></a>4、Experiments</h2><p><strong>Baseline：</strong>CornerNet</p><p><strong>Backbone：</strong>the stacked hourglass network with 52 and 104 layers</p><h3 id="4-1、Comparison"><a href="#4-1、Comparison" class="headerlink" title="4.1、Comparison"></a>4.1、Comparison</h3><p>在MS-COCO test-dev数据集上，与State-of-the-art Detectors的对比结果:</p><p><img src="6.png" alt="State-of-the-art Detectors的对比结果图"></p><p>在MS-COCO validation数据集上，检测结果定性分析：</p><p><img src="7.png" alt="检测结果定性分析图"></p><p>CornerNet和CenterNet误检率对比的结果如下：</p><p><img src="8.png" alt="误检率对比结果图"></p><p>上图显示了CenterNet避免了大量错误bounding box的检测，尤其是小尺寸的错误bounding box。</p><p>推理速度的对比：</p><ul><li><strong>average inference time of CornerNet511-104：</strong>300ms per image</li><li><strong>average inference time of CenterNet511-104：</strong>340ms per image</li><li><strong>average inference time of CenterNet511-52：</strong>270ms per image</li></ul><h3 id="4-2、Ablation-Study"><a href="#4-2、Ablation-Study" class="headerlink" title="4.2、Ablation Study"></a>4.2、Ablation Study</h3><p>Baseline:CornerNet511-52。CRE表示center region exploration。CTP表示center pooling。CCP表示cascade corner pooling。</p><p><img src="9.png" alt="消融研究图"></p><p>有无利用center信息的定性分析:</p><p><img src="10.png" alt="关于center信息定性分析图"></p><p>有无center pooling的定性分析:</p><p><img src="11.png" alt="有无center pooling的定性分析图"></p><p>在corner pooling和cascade corner pooling上的定性分析：</p><p><img src="12.png" alt="关于corner/cascade corner pooling定性分析图"></p><h3 id="4-2、Error-Analysis"><a href="#4-2、Error-Analysis" class="headerlink" title="4.2、Error Analysis"></a>4.2、Error Analysis</h3><p><img src="13.png" alt="Error Analysis图"></p><h2 id="5、Main-contribution"><a href="#5、Main-contribution" class="headerlink" title="5、Main contribution"></a>5、Main contribution</h2><p>1、该论文提出了一种CenterNet模型，将目标检测任务转换成检测一个center keypoint和两个corners，三个点的任务。<br>2、该论文设计了center pooling和cascade corner pooling两个模块，来解决CornerNet无法利用bounding box内部信息的问题。</p><p><img src="0.png" alt=""></p><blockquote><p><strong><em>“你说被火烧过、才能出现凤凰</em></strong><br> <strong><em>逆风的方向、更适合飞翔</em></strong><br> <strong><em>我不怕千万人阻挡、只怕自己投降。”   </em></strong></p><p align="right"><strong><em>——五月天《倔强》</em></strong></p><p></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> ICCV </tag>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文解读】【arXiv2019】Objects as Points</title>
      <link href="2021/01/15/cvpaper2/"/>
      <url>2021/01/15/cvpaper2/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>论文地址：</strong><a href="https://arxiv.org/pdf/1904.07850.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.07850.pdf</a><br><strong>代码地址：</strong><a href="https://github.com/xingyizhou/CenterNet" target="_blank" rel="noopener"> https://github.com/xingyizhou/CenterNet</a></p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>该论文于2019年发表在arXiv上，但该作者团队另一篇《Tracking Objects as Points》已于2020被ECCV接受，这篇已被接受的论文主要是将CenterNet网络运用在了目标跟踪的任务上。本文主要介绍在目标检测任务上的《Objects as Points》工作。</p><h2 id="1、Background"><a href="#1、Background" class="headerlink" title="1、Background"></a>1、Background</h2><h3 id="1-1、Task"><a href="#1-1、Task" class="headerlink" title="1.1、Task"></a>1.1、Task</h3><p><strong>Object Detection：</strong>找出图像中所有感兴趣的目标，确定它们的位置和类别。</p><h3 id="1-2、Problem"><a href="#1-2、Problem" class="headerlink" title="1.2、Problem"></a>1.2、Problem</h3><p>该论文对One-stage Detectors和Two-stage Detectors的方法概述：</p><ol><li>One-stage Detectors: 在图像上滑动anchors（遍历滑动窗口策略），在不指明box内容的情况下，直接对其进行分类。</li><li>Two-stage Detectors: 对于每个潜在的box再次进行特征计算，然后对这些特征进行分类。</li></ol><p><strong>Related Work:</strong><br>该论文在Related Work部分主要从Object detection by region classification（RCNN、Faster-RCNN）、Object detection with implicit anchors（Faster-RCNN）、Object detection by keypoint estimation（CornerNet、ExtremeNet）、Monocular 3D object detection（Deep3Dbox、3D RCNN）四个方面进行了简介，概述了这些的方法目前存在的缺陷，以及作者所提方法与其中类似方法的区别。</p><p><strong>Drawback：</strong></p><ol><li>基于滑动窗口的目标检测器，由于需要枚举所有有可能的目标的位置和维度信息，所以很浪费算力。</li><li>One-stage Detectors和Two-stage Detectors的方法大多需要post-processin（NMS），但这些后处理步骤是很难进行微分和训练。</li></ol><p><img src="1.png" alt="Standard anchor-based和Center point-based方法对比图"></p><h2 id="2、Motivation"><a href="#2、Motivation" class="headerlink" title="2、Motivation"></a>2、Motivation</h2><ul><li>作者将Object Detection任务转换为Keypoint Estimation任务。</li><li>关于Object Detection任务中的一些属性，例如尺寸、位置、方向和姿势等，可直接对中心点的图像特征回归得到。</li></ul><h2 id="3、Method"><a href="#3、Method" class="headerlink" title="3、Method"></a>3、Method</h2><p>该方法实现Object Detection的过程为：Input image $\Longrightarrow$ Heatmap $\Longrightarrow$ Peaks $\Longrightarrow$ Objects bounding box<br><img src="2.png" alt="Center point-based detection方法实现图"></p><h3 id="3-1-Preliminary"><a href="#3-1-Preliminary" class="headerlink" title="3.1 Preliminary"></a>3.1 Preliminary</h3><p>$\widehat{Y}$是一个keypoint heatmap，表示预测结果。$C$表示keypoint的类型数量，不同任务（人体姿态估计、目标检测等）有不同数量的类型。<br>$$\widehat{Y} \in[0,1]^{\frac WH \times \frac WH \times C} \left\{<br>\begin{array}{ccc}<br>0 &amp; &amp; background\\<br>1 &amp; &amp; detected \ keypoint<br>\end{array}<br>\right.<br>$$</p><p> $N$表示图像$I$中keypoint的数量。$\alpha$和$\beta$为超参数。Training objective如下：</p><p>$$ L_k=-\frac{1}{N}    \sum_{xyc} \left\{<br>\begin{array}{ccc}<br>(1-\widehat{Y}_{xyc})^\alpha \log (\widehat{Y}_{xyc}) &amp; &amp;if \ Y_{xyc}=1 \\<br>(1-Y_{xyc})^\beta(\widehat{Y}_{xyc})^\alpha \log(1-\widehat{Y}_{xyc}) &amp; &amp;otherwise<br>\end{array}<br>\right.<br>$$</p><p>局部offset $\widehat{O} \in R^{\frac WH \times \frac WH \times 2}$，用$L_1$Loss训练所得：<br>$$L_{off} = \frac 1N \sum_{p} \left\vert \widehat{O}_{\tilde{p}}-(\frac pR - \tilde{p}) \right\vert$$</p><h3 id="3-2、Objects-as-Points"><a href="#3-2、Objects-as-Points" class="headerlink" title="3.2、Objects as Points"></a>3.2、Objects as Points</h3><p>尺寸预测$\widehat{S} \in R^{\frac WH \times \frac WH \times 2}$，用$L_1$Loss训练所得：<br>$$L_{size} = \frac 1N \sum_{k=1}^N \left\vert \ \widehat{S}_{p_k} -s_k\right\vert$$</p><p>Training objective：<br>$$L_{det} + L_k + \lambda_{size}L_{size} + \lambda_{off}L_{off}$$</p><p>从keypoint中得到bounding box：<br>$$(\widehat{x}_i+ \delta\widehat{x}_i - \widehat{w}_i/2 , \ \widehat{y}_i+ \delta\widehat{y}_i - \widehat{h}_i/2 , \ \widehat{x}_i+ \delta\widehat{x}_i + \widehat{w}_i/2 ,\  \widehat{y}_i+ \delta\widehat{y}_i + \widehat{h}_i/2)$$</p><h3 id="3-3、Tasks"><a href="#3-3、Tasks" class="headerlink" title="3.3、Tasks"></a>3.3、Tasks</h3><p><strong>3D detection</strong><br>每个中心点还需要三个的属性：depth，3D dimension，orientation。</p><ul><li>depth：a single scalar</li><li>3D dimension：three scalars.</li><li>orientation：a single scalar by default.</li></ul><p><strong>Human pose estimation</strong><br>对图像中的每个人体实例估计$k$个 2D的人体关节位置。首选使用像素上的联合offsets直接回归可得，其次估计$k$个人体关节heatmaps，然后将最初的人体关节预测值去拟合heatmaps上最近的检测得到的keypoints上。<br><img src="3.png" alt="三种Task的具体属性要求"></p><p>该论文实验部分主要用到的Backbone如下：<br><img src="4.png" alt="所用Backbone的简介图"></p><h2 id="4、Experiments"><a href="#4、Experiments" class="headerlink" title="4、Experiments"></a>4、Experiments</h2><h3 id="4-1、Comparison"><a href="#4-1、Comparison" class="headerlink" title="4.1、Comparison"></a>4.1、Comparison</h3><p>在COCO validation上，使用不同的Backbones和测试选项所得结果：</p><p><img src="5.png" alt="COCO validation上结果比较图"></p><p>与其他Real-time detectors对比结果：</p><p><img src="6.png" alt="与其他Real-time detectors对比图"></p><p>在COCO test-dev上与其他State-of-the-art方法的对比结果：</p><p><img src="7.png" alt="与其他State-of-the-art方法的对比图"></p><p>在Pascal VOC 2007 test上的对比结果：</p><p><img src="8.png" alt="在Pascal VOC 2007 test上的对比图"></p><h3 id="4-2、Ablation-Study"><a href="#4-2、Ablation-Study" class="headerlink" title="4.2、Ablation Study"></a>4.2、Ablation Study</h3><p>在Center point collision和IoU based NMS方面：</p><p><img src="9.png" alt="Ablation Study1图"></p><p>在分辨率回归Loss方面：</p><p><img src="10.png" alt="Ablation Study2图"></p><p>在Bounding box和训练策略方面</p><p><img src="11.png" alt="Ablation Study3图"></p><h3 id="4-3、Task"><a href="#4-3、Task" class="headerlink" title="4.3、Task"></a>4.3、Task</h3><p>在KITTI evaluation上的3D detection，结果如下：</p><p><img src="12.png" alt="3D detection实验结果图"></p><p>在MS COCO dataset上的Pose estimation，结果如下：</p><p><img src="13.png" alt="Pose estimation实验结果图"></p><p>定性分析，结果如下：</p><p><img src="14.png" alt="定性分析图"></p><h2 id="5、Main-contribution"><a href="#5、Main-contribution" class="headerlink" title="5、Main contribution"></a>5、Main contribution</h2><p>1、该论文将Object Detection任务看做边界框中心点估计任务。<br>2、该论文提出了不带有例如NMS的post-processing的CenterNet方法。<br>3、该网络可以广泛地应用在其他任务上。<br>4、该方法性能优于基于边界框的检测器</p><p><img src="0.png" alt=""></p><blockquote><p><strong><em>“我对我的梦有种迷恋、大于痛苦大于这世界。”                         </em></strong></p><p align="right"><strong><em>——尤长靖《一颗星的夜》</em></strong></p> <p></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> Object Detection </tag>
            
            <tag> arXiv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文解读】【CVPR2019】Query-guided End-to-End Person Search</title>
      <link href="2021/01/14/cvpaper1/"/>
      <url>2021/01/14/cvpaper1/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>论文地址：</strong><a href="https://arxiv.org/pdf/1905.01203.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.01203.pdf</a></p></blockquote><h2 id="1、Background"><a href="#1、Background" class="headerlink" title="1、Background"></a>1、Background</h2><h3 id="1-1、Task"><a href="#1-1、Task" class="headerlink" title="1.1、Task"></a>1.1、Task</h3><p><strong>Person Search:</strong>在一个完全无约束的场景图像库中定位与query person匹配的目标。</p><h3 id="1-2、Problem"><a href="#1-2、Problem" class="headerlink" title="1.2、Problem"></a>1.2、Problem</h3><p>首先，作者坚信两点：</p><ol><li>detection和Re-id需要被实现在同一个优化框架下。</li><li>行人搜索需要充分地利用query image（尤其是query部分）的信息。</li></ol><p>其次，People Search经典的方法是将该问题分为Detection和Re-id两部分考虑，并通过分开的监督网络逐个解决。而作者提出的QEEPS是第一个端到端的query引导的Detection和Re-id网络。</p><p>最后，该论文Related Work部分主要从Person Detection、Person Re-Identification、End-to-End Person Search、Query-guided person search四部分进行介绍，提到了这四个方面里一些比较经典的工作，例如Faster R-CNN、手工特征设计、度量学习、OIM等。</p><h2 id="2、Motivation"><a href="#2、Motivation" class="headerlink" title="2、Motivation"></a>2、Motivation</h2><ul><li>在一张图像里搜索一个人，需要看重行人衣服的颜色、纹理等特别的部分。</li><li>Detection和Re-id两部分需要在一个共同优化框架内。</li></ul><h2 id="3、Method"><a href="#3、Method" class="headerlink" title="3、Method"></a>3、Method</h2><p><img src="1.png" alt="模型结构图"><br>三个子网络：</p><ol><li>QSSE-Net:a Query-guided Siamese Squeeze-and-Excitation Network<br>利用query和图像集图像中的全局上下文信息来给特征通道分配权重。</li><li>QRPN:a Query-guided Region Proposal Network（局部）<br>利用query-ROI-Pooled特征来强调图像集图像中的判别模式来产生相关的proposals。</li><li>QSimNet:a Query-guided Similarity Network （局部）<br>计算query和图像集图像里裁剪特征的Re-id得分。</li></ol><h3 id="3-1、Query-guided-Siamese-Squeeze-and-Excitation-Network-QSSE-Net"><a href="#3-1、Query-guided-Siamese-Squeeze-and-Excitation-Network-QSSE-Net" class="headerlink" title="3.1、Query-guided Siamese Squeeze-and-Excitation Network $(QSSE-Net)$"></a>3.1、Query-guided Siamese Squeeze-and-Excitation Network $(QSSE-Net)$</h3><p>利用query和图像集图像中的全局上下文信息，计算出一个权重向量，并为特征图每个通道分配权重。<br>Squeeze操作：通过全局平均池压缩query和图像集的每个C通道的空间信息。<br>Excitation操作：应用两个全连接层的和S函数，为特征图每个通道分配权重。<br><img src="2.png" alt="QSSE-Net"><br>它应用squeeze-and-Excitation操作，考虑到网络内通道的依赖性和网络间通道的相似性，重新校准query和图库图像通道权重。</p><h3 id="3-2、Query-guided-RPN-QRPN"><a href="#3-2、Query-guided-RPN-QRPN" class="headerlink" title="3.2、Query-guided RPN $(QRPN)$"></a>3.2、Query-guided RPN $(QRPN)$</h3><p><img src="3.png" alt="QRPN"><br>QRPN：query引导的逐通道的attention机制、一个标准RPN。提取具有与query相似性分数的proposal框。<br>ROI Pool：池化query裁剪特征，对池化后特征图的所有通道和像素应用FC1层，再应用excitations到图库图像特征。<br>FC1层：压缩特征。<br>FC2层：扩展特征。<br>FC1层和FC2层的作用：强调重要的信号关联信息。</p><h3 id="3-3-Query-guided-Similarity-Net-QSimNet"><a href="#3-3-Query-guided-Similarity-Net-QSimNet" class="headerlink" title="3.3  Query-guided Similarity Net $(QSimNet)$"></a>3.3  Query-guided Similarity Net $(QSimNet)$</h3><p>将query中的Re-Id特征与gallery里图像块对比。<br><img src="4.png" alt="QSimNet"><br>Re-Id特征的L2距离（逐元素的相减和平方）。<br>在推理阶段，为了query在gallery中的最终匹配图像，对该输出分数执行非极大值抑制（NMS）。</p><h3 id="3-4-End-to-end-Joint-Optimization"><a href="#3-4-End-to-end-Joint-Optimization" class="headerlink" title="3.4 End-to-end Joint Optimization"></a>3.4 End-to-end Joint Optimization</h3><p>$$L=\lambda_1L_{cls}+\lambda_2L_{reg}+\lambda_3L_{rpn_o}+\lambda_4L_{rpn_r}+\lambda_{5}L_{oim}+\lambda_6L{qrpn}+\lambda_7L_{sim}$$<br>$$L_{qrpn}=-\frac{1}{N}\sum_{N}\log(p_n^u)$$</p><h2 id="4、Experiments"><a href="#4、Experiments" class="headerlink" title="4、Experiments"></a>4、Experiments</h2><h3 id="4-1、Ablation-Study"><a href="#4-1、Ablation-Study" class="headerlink" title="4.1、Ablation Study"></a>4.1、Ablation Study</h3><p><img src="5.png" alt=""><br><img src="6.png" alt=""><br><img src="7.png" alt=""></p><h2 id="5、Main-contribution"><a href="#5、Main-contribution" class="headerlink" title="5、Main contribution"></a>5、Main contribution</h2><p>1、引入了第一个query引导的端到端的网络$(QEEPS)$。<br>2、提出了一个query引导的Siamese squeeze-and-excitation$(QSSE)$模块（对query和图像库图像对之间的相似性建模）。<br>3、定义一个新的query引导的$(QRPN)$，将SE-Net扩展到query通道和空间通道。<br>4、定义了一个新的QSimNet，学习query引导的Re-Id分数。</p><p><img src="0.png" alt=""></p><blockquote><p><strong><em>“这一生志愿只要平凡快乐、谁说这样不伟大呢。”                         </em></strong></p><p align="right"><strong><em>——五月天《笑忘歌》</em></strong></p> <p></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> CVPR </tag>
            
            <tag> Person Search </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
